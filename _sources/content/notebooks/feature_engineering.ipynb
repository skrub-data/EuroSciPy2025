{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef45910a",
   "metadata": {},
   "source": [
    "# Feature engineering for electricity load forecasting\n",
    "\n",
    "The purpose of this notebook is to demonstrate how to use `skrub` and\n",
    "`polars` to perform feature engineering for electricity load forecasting.\n",
    "\n",
    "We will build a set of features (and targets) from different data sources:\n",
    "\n",
    "- Historical weather data for 10 medium to large urban areas in France;\n",
    "- Holidays and standard calendar features for France;\n",
    "- Historical electricity load data for the whole of France.\n",
    "\n",
    "All these data sources cover a time range from March 23, 2021 to May 31,\n",
    "2025.\n",
    "\n",
    "Since our forecasting horizon is 24 hours, we consider that the\n",
    "future weather data is known at a chosen prediction time. Similarly, the\n",
    "holidays and calendar features are known at prediction time for any point in\n",
    "the future.\n",
    "We can also use the load data to engineer some lagged features and rolling\n",
    "aggregations.\n",
    "\n",
    " The future values of the load\n",
    "data (with respect to the prediction time) are used as targets for the\n",
    "forecasting model.\n",
    "\n",
    "## Environment setup\n",
    "\n",
    "We need to install some extra dependencies for this notebook if needed (when\n",
    "running jupyterlite)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80482ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q https://pypi.anaconda.org/ogrisel/simple/polars/1.24.0/polars-1.24.0-cp39-abi3-emscripten_3_1_58_wasm32.whl\n",
    "%pip install -q altair holidays plotly nbformat skrub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff6094e",
   "metadata": {},
   "source": [
    "\n",
    "The following 3 imports are only needed to workaround some limitations when\n",
    "using polars in a pyodide/jupyterlite notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a4c6e6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import tzdata  # noqa: F401\n",
    "import pandas as pd\n",
    "from pyarrow.parquet import read_table\n",
    "\n",
    "import altair\n",
    "import polars as pl\n",
    "import skrub\n",
    "from pathlib import Path\n",
    "import holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85f5739",
   "metadata": {},
   "source": [
    "## Shared time range for all historical data sources\n",
    "\n",
    "Let's define a hourly time range from March 23, 2021 to May 31, 2025 that\n",
    "will be used to join the electricity load data and the weather data. The time\n",
    "range is in UTC timezone to avoid any ambiguity when joining with the weather\n",
    "data that is also in UTC.\n",
    "\n",
    "We wrap the resulting polars dataframe in a `skrub` DataOp to benefit\n",
    "from the built-in `skrub.TableReport` display in the notebook. Using the\n",
    "`skrub` DataOps will also be useful for other reasons: all\n",
    "operations in this notebook are chained together in a directed\n",
    "acyclic graph that is automatically tracked by `skrub`. This allows us to\n",
    "extract the resulting pipeline and apply it to new data later on, exactly\n",
    "like a trained scikit-learn pipeline. The main difference is that we do so\n",
    "incrementally and while eagerly executing and inspecting the results of each\n",
    "step as is customary when working with dataframe libraries such as polars and\n",
    "pandas in Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51242dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_data_start_time = skrub.var(\n",
    "    \"historical_data_start_time\", pl.datetime(2021, 3, 23, hour=0, time_zone=\"UTC\")\n",
    ")\n",
    "historical_data_end_time = skrub.var(\n",
    "    \"historical_data_end_time\", pl.datetime(2025, 5, 31, hour=23, time_zone=\"UTC\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9af0277",
   "metadata": {},
   "outputs": [],
   "source": [
    "@skrub.deferred\n",
    "def build_historical_time_range(\n",
    "    historical_data_start_time,\n",
    "    historical_data_end_time,\n",
    "    time_interval=\"1h\",\n",
    "    time_zone=\"UTC\",\n",
    "):\n",
    "    \"\"\"Define an historical time range shared by all data sources.\"\"\"\n",
    "    return pl.DataFrame().with_columns(\n",
    "        pl.datetime_range(\n",
    "            start=historical_data_start_time,\n",
    "            end=historical_data_end_time,\n",
    "            time_zone=time_zone,\n",
    "            interval=time_interval,\n",
    "        ).alias(\"time\"),\n",
    "    )\n",
    "\n",
    "\n",
    "time = build_historical_time_range(historical_data_start_time, historical_data_end_time)\n",
    "time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5854ea",
   "metadata": {},
   "source": [
    "\n",
    "If you run the above locally with pydot and graphviz installed, you can\n",
    "visualize the expression graph of the `time` variable by expanding the \"Show\n",
    "graph\" button.\n",
    "\n",
    "Let's now load the data records for the time range defined above.\n",
    "\n",
    "To avoid network issues when running this notebook, the necessary data files\n",
    "have already been downloaded and saved in the `datasets` folder. See the\n",
    "README.md file for instructions to download the data manually if you want to\n",
    "re-run this notebook with more recent data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5278fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_folder = skrub.var(\"data_source_folder\", \"../datasets\")\n",
    "\n",
    "for data_file in sorted(Path(data_source_folder.skb.eval()).iterdir()):\n",
    "    print(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507ad972",
   "metadata": {},
   "source": [
    "\n",
    "We define a list of 10 medium to large urban areas to approximately cover\n",
    "most regions in France with a slight focus on most populated regions that are\n",
    "likely to drive electricity demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeddcfa6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "city_names = skrub.var(\n",
    "    \"city_names\",\n",
    "    [\n",
    "        \"paris\",\n",
    "        \"lyon\",\n",
    "        \"marseille\",\n",
    "        \"toulouse\",\n",
    "        \"lille\",\n",
    "        \"limoges\",\n",
    "        \"nantes\",\n",
    "        \"strasbourg\",\n",
    "        \"brest\",\n",
    "        \"bayonne\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "@skrub.deferred\n",
    "def load_weather_data(time, city_names, data_source_folder):\n",
    "    \"\"\"Load and horizontal stack historical weather forecast data for each city.\"\"\"\n",
    "    all_city_weather = time\n",
    "    for city_name in city_names:\n",
    "        all_city_weather = all_city_weather.join(\n",
    "            pl.from_arrow(\n",
    "                read_table(f\"{data_source_folder}/weather_{city_name}.parquet\")\n",
    "            )\n",
    "            .with_columns([pl.col(\"time\").dt.cast_time_unit(\"us\")])\n",
    "            .rename(lambda x: x if x == \"time\" else \"weather_\" + x + \"_\" + city_name),\n",
    "            on=\"time\",\n",
    "        )\n",
    "    return all_city_weather\n",
    "\n",
    "\n",
    "all_city_weather = load_weather_data(time, city_names, data_source_folder)\n",
    "all_city_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5eb1b6",
   "metadata": {},
   "source": [
    "## Calendar and holidays features\n",
    "\n",
    "We leverage the `holidays` package to enrich the time range with some calendar\n",
    "features such as public holidays in France. We also add some features that are useful\n",
    "for time series forecasting such as the day of the week, the day of the year, and the\n",
    "hour of the day.\n",
    "\n",
    "We want to use the `holidays` package to enrich the time range with some calendar\n",
    "features such as public holidays in France. In addition, we want to use `skrub`\n",
    "`DatetimeEncoder` to add some features that are useful for time series forecasting\n",
    "such as the calendar year, month, day, hour, the day of the week and the day of the\n",
    "year.\n",
    "\n",
    "Note that the `holidays` package requires us to extract the date for the French\n",
    "timezone.\n",
    "\n",
    "Similarly for the calendar features: all the time features are extracted from the time\n",
    "in the French timezone, since it is likely that electricity usage patterns are\n",
    "influenced by inhabitants' daily routines aligned with the local timezone.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Let's first create some calendar features using `skrub`'s `DatetimeEncoder`.\n",
    "\n",
    "1. Create a `DatetimeEncoder` object and by looking at the documentation, make sure\n",
    "   to add the weekday and the day of the year. Do not add the total seconds since the\n",
    "   Unix epoch. You can refer to this link:\n",
    "   https://skrub-data.org/stable/reference/generated/skrub.DatetimeEncoder.html\n",
    "2. As a first operation, we wish to rename the `time` column to `cal` such that\n",
    "   the all columns corresponding to some calendar features will be prefixed with\n",
    "   `cal_`. You can simply call the `rename` method (cf.\n",
    "   https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.rename.html)\n",
    "   because `time` can be seen as a polars dataframe.\n",
    "3. Now, we wish to apply the encoder to the `time` dataframe. Refer to the following\n",
    "   link for all details:\n",
    "   https://skrub-data.org/stable/reference/generated/skrub.DataOp.skb.apply.html\n",
    "4. Let's call the resulting skrub `DataOp` `time_encoded` and check the output\n",
    "   representation to check if the preview looks what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac1f17",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from skrub import DatetimeEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b90e83",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1670b33",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "datetime_encoder = DatetimeEncoder(\n",
    "    add_weekday=True, add_day_of_year=True, add_total_seconds=False\n",
    ")\n",
    "time_encoded = time.rename({\"time\": \"cal\"}).skb.apply(datetime_encoder)\n",
    "time_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50b532d",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise\n",
    "\n",
    "Now, let's create a processing function that is going to be decorated with the\n",
    "`@skrub.deferred` decorator. This function should:\n",
    "1. Take the `time` dataframe as an input.\n",
    "2. Convert the \"time\" column to the French/Paris timezone.\n",
    "3. Extract the French holidays by calling `holidays.country_holidays`. For this\n",
    "   function, you need to extract the minimum and maximum year from the \"time\" column.\n",
    "4. Finally, you need to if a date in holiday is a French holiday. You can call this\n",
    "   column `cal_is_holiday`.\n",
    "5. Apply this function to the `time` `DataOp` and call the resulting variable\n",
    "   `is_french_holiday`.\n",
    "6. Finally, we wish to concatenate the `time_encoded` and `is_french_holiday` using\n",
    "   the `.skb.concat` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285c0263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "342c855f",
   "metadata": {},
   "source": [
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f841553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@skrub.deferred\n",
    "def prepare_holidays(time):\n",
    "    fr_time = pl.col(\"time\").dt.convert_time_zone(\"Europe/Paris\")\n",
    "    fr_year_min = time.select(fr_time.dt.year().min()).item()\n",
    "    fr_year_max = time.select(fr_time.dt.year().max()).item()\n",
    "    holidays_fr = holidays.country_holidays(\n",
    "        \"FR\", years=range(fr_year_min, fr_year_max + 1)\n",
    "    )\n",
    "    return time.select(\n",
    "        fr_time.dt.date().is_in(holidays_fr.keys()).alias(\"cal_is_holiday\"),\n",
    "    )\n",
    "\n",
    "is_french_holiday = prepare_holidays(time)\n",
    "is_french_holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b57a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = time.skb.concat([time_encoded, is_french_holiday], axis=1)\n",
    "calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0825b2c7",
   "metadata": {},
   "source": [
    "\n",
    "## Electricity load data\n",
    "\n",
    "Finally we load the electricity load data. This data will both be used as a\n",
    "target variable but also to craft some lagged and window-aggregated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb611950",
   "metadata": {},
   "outputs": [],
   "source": [
    "@skrub.deferred\n",
    "def load_electricity_load_data(time, data_source_folder):\n",
    "    \"\"\"Load and aggregate historical load data from the raw CSV files.\"\"\"\n",
    "    load_data_files = [\n",
    "        data_file\n",
    "        for data_file in sorted(Path(data_source_folder).iterdir())\n",
    "        if data_file.name.startswith(\"Total Load - Day Ahead\")\n",
    "        and data_file.name.endswith(\".csv\")\n",
    "    ]\n",
    "    return time.join(\n",
    "        (\n",
    "            pl.concat(\n",
    "                [\n",
    "                    pl.from_pandas(pd.read_csv(data_file, na_values=[\"N/A\", \"-\"])).drop(\n",
    "                        [\"Day-ahead Total Load Forecast [MW] - BZN|FR\"]\n",
    "                    )\n",
    "                    for data_file in load_data_files\n",
    "                ]\n",
    "            ).select(\n",
    "                [\n",
    "                    pl.col(\"Time (UTC)\")\n",
    "                    .str.split(by=\" - \")\n",
    "                    .list.first()\n",
    "                    .str.to_datetime(\"%d.%m.%Y %H:%M\", time_zone=\"UTC\")\n",
    "                    .alias(\"time\"),\n",
    "                    pl.col(\"Actual Total Load [MW] - BZN|FR\").alias(\"load_mw\"),\n",
    "                ]\n",
    "            )\n",
    "        ),\n",
    "        on=\"time\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fd5964",
   "metadata": {},
   "source": [
    "\n",
    "Let's load the data and check if there are missing values since we will use\n",
    "this data as the target variable for our forecasting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7816e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_raw = load_electricity_load_data(time, data_source_folder)\n",
    "electricity_raw.filter(pl.col(\"load_mw\").is_null())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00076e9",
   "metadata": {},
   "source": [
    "\n",
    "So apparently there a few missing measurements. Let's use linear\n",
    "interpolation to fill those missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61800c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_raw.filter(\n",
    "    (pl.col(\"time\") > pl.datetime(2021, 10, 30, hour=10, time_zone=\"UTC\"))\n",
    "    & (pl.col(\"time\") < pl.datetime(2021, 10, 31, hour=10, time_zone=\"UTC\"))\n",
    ").skb.eval().plot.line(x=\"time:T\", y=\"load_mw:Q\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95d03e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity = electricity_raw.with_columns([pl.col(\"load_mw\").interpolate()])\n",
    "electricity.filter(\n",
    "    (pl.col(\"time\") > pl.datetime(2021, 10, 30, hour=10, time_zone=\"UTC\"))\n",
    "    & (pl.col(\"time\") < pl.datetime(2021, 10, 31, hour=10, time_zone=\"UTC\"))\n",
    ").skb.eval().plot.line(x=\"time:T\", y=\"load_mw:Q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fe3b4a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "## Lagged features\n",
    "\n",
    "We can now create some lagged features from the electricity load data.\n",
    "\n",
    "We will create 3 hourly lagged features, 1 daily lagged feature, and 1 weekly\n",
    "lagged feature. We will also create a rolling median and inter-quartile\n",
    "feature over the last 24 hours and over the last 7 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cd21d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr(col, *, window_size: int):\n",
    "    \"\"\"Inter-quartile range (IQR) of a column.\"\"\"\n",
    "    return col.rolling_quantile(0.75, window_size=window_size) - col.rolling_quantile(\n",
    "        0.25, window_size=window_size\n",
    "    )\n",
    "\n",
    "\n",
    "electricity_lagged = electricity.with_columns(\n",
    "    [pl.col(\"load_mw\").shift(i).alias(f\"load_mw_lag_{i}h\") for i in range(1, 4)]\n",
    "    + [\n",
    "        pl.col(\"load_mw\").shift(24).alias(\"load_mw_lag_1d\"),\n",
    "        pl.col(\"load_mw\").shift(24 * 7).alias(\"load_mw_lag_1w\"),\n",
    "        pl.col(\"load_mw\")\n",
    "        .rolling_median(window_size=24)\n",
    "        .alias(\"load_mw_rolling_median_24h\"),\n",
    "        pl.col(\"load_mw\")\n",
    "        .rolling_median(window_size=24 * 7)\n",
    "        .alias(\"load_mw_rolling_median_7d\"),\n",
    "        iqr(pl.col(\"load_mw\"), window_size=24).alias(\"load_mw_iqr_24h\"),\n",
    "        iqr(pl.col(\"load_mw\"), window_size=24 * 7).alias(\"load_mw_iqr_7d\"),\n",
    "    ],\n",
    ")\n",
    "electricity_lagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140e422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "altair.Chart(electricity_lagged.tail(100).skb.preview()).transform_fold(\n",
    "    [\n",
    "        \"load_mw\",\n",
    "        \"load_mw_lag_1h\",\n",
    "        \"load_mw_lag_2h\",\n",
    "        \"load_mw_lag_3h\",\n",
    "        \"load_mw_lag_1d\",\n",
    "        \"load_mw_lag_1w\",\n",
    "        \"load_mw_rolling_median_24h\",\n",
    "        \"load_mw_rolling_median_7d\",\n",
    "        \"load_mw_rolling_iqr_24h\",\n",
    "        \"load_mw_rolling_iqr_7d\",\n",
    "    ],\n",
    "    as_=[\"key\", \"load_mw\"],\n",
    ").mark_line(tooltip=True).encode(x=\"time:T\", y=\"load_mw:Q\", color=\"key:N\").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f8a9c9",
   "metadata": {},
   "source": [
    "## Final dataset\n",
    "\n",
    "We now assemble the dataset that will be used to train and evaluate the forecasting\n",
    "models via backtesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fab1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_start_time = skrub.var(\n",
    "    \"prediction_start_time\", historical_data_start_time.skb.eval() + pl.duration(days=7)\n",
    ")\n",
    "prediction_end_time = skrub.var(\n",
    "    \"prediction_end_time\", historical_data_end_time.skb.eval() - pl.duration(hours=24)\n",
    ")\n",
    "\n",
    "\n",
    "@skrub.deferred\n",
    "def define_prediction_time_range(prediction_start_time, prediction_end_time):\n",
    "    return pl.DataFrame().with_columns(\n",
    "        pl.datetime_range(\n",
    "            start=prediction_start_time,\n",
    "            end=prediction_end_time,\n",
    "            time_zone=\"UTC\",\n",
    "            interval=\"1h\",\n",
    "        ).alias(\"prediction_time\"),\n",
    "    )\n",
    "\n",
    "\n",
    "prediction_time = define_prediction_time_range(\n",
    "    prediction_start_time, prediction_end_time\n",
    ").skb.subsample(n=1000, how=\"head\")\n",
    "prediction_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7494f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@skrub.deferred\n",
    "def build_features(\n",
    "    prediction_time,\n",
    "    electricity_lagged,\n",
    "    all_city_weather,\n",
    "    calendar,\n",
    "    future_feature_horizons=[1, 24],\n",
    "):\n",
    "\n",
    "    return (\n",
    "        prediction_time.join(\n",
    "            electricity_lagged, left_on=\"prediction_time\", right_on=\"time\"\n",
    "        )\n",
    "        .join(\n",
    "            all_city_weather.select(\n",
    "                [pl.col(\"time\")]\n",
    "                + [\n",
    "                    pl.col(c).shift(-h).alias(c + f\"_future_{h}h\")\n",
    "                    for c in all_city_weather.columns\n",
    "                    if c != \"time\"\n",
    "                    for h in future_feature_horizons\n",
    "                ]\n",
    "            ),\n",
    "            left_on=\"prediction_time\",\n",
    "            right_on=\"time\",\n",
    "        )\n",
    "        .join(\n",
    "            calendar.select(\n",
    "                [pl.col(\"time\")]\n",
    "                + [\n",
    "                    pl.col(c).shift(-h).alias(c + f\"_future_{h}h\")\n",
    "                    for c in calendar.columns\n",
    "                    if c != \"time\"\n",
    "                    for h in future_feature_horizons\n",
    "                ]\n",
    "            ),\n",
    "            left_on=\"prediction_time\",\n",
    "            right_on=\"time\",\n",
    "        )\n",
    "    ).drop(\"prediction_time\")\n",
    "\n",
    "\n",
    "features = build_features(\n",
    "    prediction_time=prediction_time,\n",
    "    electricity_lagged=electricity_lagged,\n",
    "    all_city_weather=all_city_weather,\n",
    "    calendar=calendar,\n",
    ").skb.mark_as_X()\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5115b02",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "Let's build training and evaluation targets for all possible horizons from 1\n",
    "to 24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a599e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "@skrub.deferred\n",
    "def build_targets(prediction_time, electricity):\n",
    "    return prediction_time.join(\n",
    "        electricity.with_columns(\n",
    "            pl.col(\"load_mw\").shift(-24).alias(\"load_mw_horizon_24h\")\n",
    "        ),\n",
    "        left_on=\"prediction_time\",\n",
    "        right_on=\"time\",\n",
    "    )\n",
    "\n",
    "\n",
    "targets = build_targets(prediction_time, electricity)\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459ba393",
   "metadata": {},
   "source": [
    "\n",
    "Let's serialize this data loading and feature engineering pipeline for\n",
    "reuse in later notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f305e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "\n",
    "with open(\"feature_engineering_pipeline.pkl\", \"wb\") as f:\n",
    "    cloudpickle.dump(\n",
    "        {\n",
    "            \"features\": features,\n",
    "            \"targets\": targets,\n",
    "            \"prediction_time\": prediction_time,\n",
    "        },\n",
    "        f,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d277e11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
