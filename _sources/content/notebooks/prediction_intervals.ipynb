{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e09b6e95",
   "metadata": {},
   "source": [
    "\n",
    "# Computing prediction intervals using quantile regression\n",
    "\n",
    "## Environment setup\n",
    "\n",
    "We need to install some extra dependencies for this notebook if needed (when\n",
    "running jupyterlite). We need the development version of skrub to be able to\n",
    "use the skrub expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b24548",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q https://pypi.anaconda.org/ogrisel/simple/polars/1.24.0/polars-1.24.0-cp39-abi3-emscripten_3_1_58_wasm32.whl\n",
    "%pip install -q https://pypi.anaconda.org/ogrisel/simple/skrub/0.6.dev0/skrub-0.6.dev0-py3-none-any.whl\n",
    "%pip install -q altair holidays plotly nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f2ae20",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import altair\n",
    "import cloudpickle\n",
    "import numpy as np\n",
    "import pyarrow  # noqa: F401\n",
    "import polars as pl\n",
    "import tzdata  # noqa: F401\n",
    "\n",
    "from tutorial_helpers import (\n",
    "    binned_coverage,\n",
    "    plot_lorenz_curve,\n",
    "    plot_reliability_diagram,\n",
    "    plot_residuals_vs_predicted,\n",
    "    collect_cv_predictions,\n",
    ")\n",
    "\n",
    "# Ignore warnings from pkg_resources triggered by Python 3.13's multiprocessing.\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pkg_resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb9dcd0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "with open(\"feature_engineering_pipeline.pkl\", \"rb\") as f:\n",
    "    feature_engineering_pipeline = cloudpickle.load(f)\n",
    "\n",
    "\n",
    "features = feature_engineering_pipeline[\"features\"]\n",
    "targets = feature_engineering_pipeline[\"targets\"]\n",
    "prediction_time = feature_engineering_pipeline[\"prediction_time\"]\n",
    "horizons = feature_engineering_pipeline[\"horizons\"]\n",
    "target_column_name_pattern = feature_engineering_pipeline[\"target_column_name_pattern\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679b73fb",
   "metadata": {},
   "source": [
    "### Define the quantile regressors\n",
    "\n",
    "In this section, we show how one can use a gradient boosting but modify the loss\n",
    "function to predict different quantiles and thus obtain an uncertainty quantification\n",
    "of the predictions.\n",
    "\n",
    "In terms of evaluation, we reuse the R2 and MAPE scores. However, they are not helpful\n",
    "to assess the reliability of quantile models. For this purpose, we use a derivate of\n",
    "the metric minimize by those models: the pinball loss. We use the D2 score that is\n",
    "easier to interpret since the best possible score is bounded by 1 and a score of 0\n",
    "corresponds to constant predictions at the target quantile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98428a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_of_interest = horizons[-1]  # Focus on the 24-hour horizon\n",
    "target_column_name = target_column_name_pattern.format(horizon=horizon_of_interest)\n",
    "predicted_target_column_name = \"predicted_\" + target_column_name\n",
    "target = targets[target_column_name].skb.mark_as_y()\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0287d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import get_scorer, make_scorer\n",
    "from sklearn.metrics import mean_absolute_percentage_error, d2_pinball_score\n",
    "\n",
    "scoring = {\n",
    "    \"r2\": get_scorer(\"r2\"),\n",
    "    \"mape\": make_scorer(mean_absolute_percentage_error),\n",
    "    \"d2_pinball_05\": make_scorer(d2_pinball_score, alpha=0.05),\n",
    "    \"d2_pinball_50\": make_scorer(d2_pinball_score, alpha=0.50),\n",
    "    \"d2_pinball_95\": make_scorer(d2_pinball_score, alpha=0.95),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d1abf3",
   "metadata": {},
   "source": [
    "\n",
    "We know define three different models:\n",
    "\n",
    "- a model predicting the 5th percentile of the load\n",
    "- a model predicting the median of the load\n",
    "- a model predicting the 95th percentile of the load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa79ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "\n",
    "common_params = dict(\n",
    "    loss=\"quantile\", learning_rate=0.1, max_leaf_nodes=100, random_state=0\n",
    ")\n",
    "predictions_hgbr_05 = features.skb.apply(\n",
    "    HistGradientBoostingRegressor(**common_params, quantile=0.05),\n",
    "    y=target,\n",
    ")\n",
    "predictions_hgbr_50 = features.skb.apply(\n",
    "    HistGradientBoostingRegressor(**common_params, quantile=0.5),\n",
    "    y=target,\n",
    ")\n",
    "predictions_hgbr_95 = features.skb.apply(\n",
    "    HistGradientBoostingRegressor(**common_params, quantile=0.95),\n",
    "    y=target,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb52edc6",
   "metadata": {},
   "source": [
    "\n",
    "### Evaluation via cross-validation\n",
    "\n",
    "We evaluate the performance of the quantile regressors via cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7425001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "max_train_size = 2 * 52 * 24 * 7  # max ~2 years of training data\n",
    "test_size = 24 * 7 * 24  # 24 weeks of test data\n",
    "gap = 7 * 24  # 1 week gap between train and test sets\n",
    "ts_cv_5 = TimeSeriesSplit(\n",
    "    n_splits=5, max_train_size=max_train_size, test_size=test_size, gap=gap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be4d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_hgbr_05 = predictions_hgbr_05.skb.cross_validate(\n",
    "    cv=ts_cv_5,\n",
    "    scoring=scoring,\n",
    "    return_pipeline=True,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "cv_results_hgbr_50 = predictions_hgbr_50.skb.cross_validate(\n",
    "    cv=ts_cv_5,\n",
    "    scoring=scoring,\n",
    "    return_pipeline=True,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "cv_results_hgbr_95 = predictions_hgbr_95.skb.cross_validate(\n",
    "    cv=ts_cv_5,\n",
    "    scoring=scoring,\n",
    "    return_pipeline=True,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a0403b",
   "metadata": {},
   "source": [
    "\n",
    "Let's first collect all the cross-validated predictions to make further inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd95030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_predictions_hgbr_05 = collect_cv_predictions(\n",
    "    cv_results_hgbr_05[\"pipeline\"], ts_cv_5, predictions_hgbr_05, prediction_time\n",
    ")\n",
    "cv_predictions_hgbr_50 = collect_cv_predictions(\n",
    "    cv_results_hgbr_50[\"pipeline\"], ts_cv_5, predictions_hgbr_50, prediction_time\n",
    ")\n",
    "cv_predictions_hgbr_95 = collect_cv_predictions(\n",
    "    cv_results_hgbr_95[\"pipeline\"], ts_cv_5, predictions_hgbr_95, prediction_time\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1c75c6",
   "metadata": {},
   "source": [
    "\n",
    "Now, let's make a plot of the predictions for each model and thus we need to gather\n",
    "all the predictions in a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8fd750",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pl.concat(\n",
    "    [\n",
    "        cv_predictions_hgbr_05[0].rename({\"predicted_load_mw\": \"predicted_load_mw_05\"}),\n",
    "        cv_predictions_hgbr_50[0].select(\"predicted_load_mw\").rename(\n",
    "            {\"predicted_load_mw\": \"predicted_load_mw_50\"}\n",
    "        ),\n",
    "        cv_predictions_hgbr_95[0].select(\"predicted_load_mw\").rename(\n",
    "            {\"predicted_load_mw\": \"predicted_load_mw_95\"}\n",
    "        ),\n",
    "    ],\n",
    "    how=\"horizontal\",\n",
    ").tail(24 * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9a4c20",
   "metadata": {},
   "source": [
    "\n",
    "Now, we plot the observed values and the predicted median with a line. In addition,\n",
    "we plot the 5th and 95th percentiles as a shaded area. It means that between those\n",
    "two bounds, we expect to find 90% of the observed values.\n",
    "\n",
    "We plot this information on a portion of the test data from the first fold of the\n",
    "cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81a60b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_chart = (\n",
    "    altair.Chart(results)\n",
    "    .transform_fold([\"load_mw\", \"predicted_load_mw_50\"])\n",
    "    .mark_line(tooltip=True)\n",
    "    .encode(x=\"prediction_time:T\", y=\"value:Q\", color=\"key:N\")\n",
    ")\n",
    "\n",
    "# Add a column for the band legend\n",
    "results_with_band = results.with_columns(pl.lit(\"90% interval\").alias(\"band_type\"))\n",
    "\n",
    "quantile_band_chart = (\n",
    "    altair.Chart(results_with_band)\n",
    "    .mark_area(opacity=0.4, tooltip=True)\n",
    "    .encode(\n",
    "        x=\"prediction_time:T\",\n",
    "        y=\"predicted_load_mw_05:Q\",\n",
    "        y2=\"predicted_load_mw_95:Q\",\n",
    "        color=altair.Color(\"band_type:N\", scale=altair.Scale(range=[\"lightgreen\"])),\n",
    "    )\n",
    ")\n",
    "\n",
    "combined_chart = quantile_band_chart + median_chart\n",
    "combined_chart.resolve_scale(color=\"independent\").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b57258",
   "metadata": {},
   "source": [
    "\n",
    "Now, we can inspect the cross-validated metrics for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda6d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_hgbr_05[\n",
    "    [col for col in cv_results_hgbr_05.columns if col.startswith(\"test_\")]\n",
    "].mean(axis=0).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b96932",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_hgbr_50[\n",
    "    [col for col in cv_results_hgbr_50.columns if col.startswith(\"test_\")]\n",
    "].mean(axis=0).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3780b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_hgbr_95[\n",
    "    [col for col in cv_results_hgbr_95.columns if col.startswith(\"test_\")]\n",
    "].mean(axis=0).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b51beb",
   "metadata": {},
   "source": [
    "\n",
    "Focusing on the different D2 scores, we observe that each model minimize the D2 score\n",
    "associated to the target quantile that we set. For instance, the model predicting the\n",
    "5th percentile obtained the highest D2 pinball score with `alpha=0.05`. It is expected\n",
    "but a confirmation of what loss each model minimizes.\n",
    "\n",
    "Now, let's collect the cross-validated predictions and plot the residual vs predicted\n",
    "values for the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dffc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals_vs_predicted(cv_predictions_hgbr_05).interactive().properties(\n",
    "    title=(\n",
    "        \"Residuals vs Predicted Values from cross-validation predictions\"\n",
    "        \" for quantile 0.05\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56466b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals_vs_predicted(cv_predictions_hgbr_50).interactive().properties(\n",
    "    title=(\"Residuals vs Predicted Values from cross-validation predictions for median\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668c1452",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals_vs_predicted(cv_predictions_hgbr_95).interactive().properties(\n",
    "    title=(\n",
    "        \"Residuals vs Predicted Values from cross-validation predictions\"\n",
    "        \" for quantile 0.95\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364e3cbc",
   "metadata": {},
   "source": [
    "\n",
    "We observe an expected behaviour: the residuals are centered and symmetric around 0\n",
    "for the median model while not centered and biased for the 5th and 95th percentiles\n",
    "models.\n",
    "\n",
    "Note that we could obtain similar plots using scikit-learn's `PredictionErrorDisplay`.\n",
    "This display allows to also plot the observed values vs predicted values as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7630eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_predictions_hgbr_05_concat = pl.concat(cv_predictions_hgbr_05, how=\"vertical\")\n",
    "cv_predictions_hgbr_50_concat = pl.concat(cv_predictions_hgbr_50, how=\"vertical\")\n",
    "cv_predictions_hgbr_95_concat = pl.concat(cv_predictions_hgbr_95, how=\"vertical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dc26a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "\n",
    "\n",
    "for kind in [\"actual_vs_predicted\", \"residual_vs_predicted\"]:\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "\n",
    "    PredictionErrorDisplay.from_predictions(\n",
    "        y_true=cv_predictions_hgbr_05_concat[\"load_mw\"].to_numpy(),\n",
    "        y_pred=cv_predictions_hgbr_05_concat[\"predicted_load_mw\"].to_numpy(),\n",
    "        kind=kind,\n",
    "        ax=axs[0],\n",
    "    )\n",
    "    axs[0].set_title(\"0.05 quantile regression\")\n",
    "\n",
    "    PredictionErrorDisplay.from_predictions(\n",
    "        y_true=cv_predictions_hgbr_50_concat[\"load_mw\"].to_numpy(),\n",
    "        y_pred=cv_predictions_hgbr_50_concat[\"predicted_load_mw\"].to_numpy(),\n",
    "        kind=kind,\n",
    "        ax=axs[1],\n",
    "    )\n",
    "    axs[1].set_title(\"Median regression\")\n",
    "\n",
    "    PredictionErrorDisplay.from_predictions(\n",
    "        y_true=cv_predictions_hgbr_95_concat[\"load_mw\"].to_numpy(),\n",
    "        y_pred=cv_predictions_hgbr_95_concat[\"predicted_load_mw\"].to_numpy(),\n",
    "        kind=kind,\n",
    "        ax=axs[2],\n",
    "    )\n",
    "    axs[2].set_title(\"0.95 quantile regression\")\n",
    "\n",
    "    fig.suptitle(f\"{kind} for GBRT minimzing different quantile losses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529fcda9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "Those plots carry the same information than the previous ones.\n",
    "\n",
    "Now, we assess if the actual coverage of the models is close to the target coverage of\n",
    "90%. In addition, we compute the average width of the bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a344bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage(y_true, y_quantile_low, y_quantile_high):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_quantile_low = np.asarray(y_quantile_low)\n",
    "    y_quantile_high = np.asarray(y_quantile_high)\n",
    "    return float(\n",
    "        np.logical_and(y_true >= y_quantile_low, y_true <= y_quantile_high)\n",
    "        .mean()\n",
    "        .round(4)\n",
    "    )\n",
    "\n",
    "\n",
    "def mean_width(y_true, y_quantile_low, y_quantile_high):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_quantile_low = np.asarray(y_quantile_low)\n",
    "    y_quantile_high = np.asarray(y_quantile_high)\n",
    "    return float(np.abs(y_quantile_high - y_quantile_low).mean().round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbad9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage(\n",
    "    cv_predictions_hgbr_50_concat[\"load_mw\"].to_numpy(),\n",
    "    cv_predictions_hgbr_05_concat[\"predicted_load_mw\"].to_numpy(),\n",
    "    cv_predictions_hgbr_95_concat[\"predicted_load_mw\"].to_numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923c5f65",
   "metadata": {},
   "source": [
    "\n",
    "We see that the obtained coverage (~77%) on the cross-validated predictions is much\n",
    "lower than the target coverage of 90%. It means that the pair of regressors is not\n",
    "jointly calibrated to estimate the 90% interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369eea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_width(\n",
    "    cv_predictions_hgbr_50_concat[\"load_mw\"].to_numpy(),\n",
    "    cv_predictions_hgbr_05_concat[\"predicted_load_mw\"].to_numpy(),\n",
    "    cv_predictions_hgbr_95_concat[\"predicted_load_mw\"].to_numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f89cf",
   "metadata": {},
   "source": [
    "\n",
    "In terms of interpretable measure, the mean width provides a measure in the original\n",
    "unit of the target variable in MW that is ~5,100 MW.\n",
    "\n",
    "We can go a bit further and bin the cross-validated predictions and check if some\n",
    "specific bins show a better or worse coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b018b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_coverage_results = binned_coverage(\n",
    "    [df[\"load_mw\"].to_numpy() for df in cv_predictions_hgbr_50],\n",
    "    [df[\"predicted_load_mw\"].to_numpy() for df in cv_predictions_hgbr_05],\n",
    "    [df[\"predicted_load_mw\"].to_numpy() for df in cv_predictions_hgbr_95],\n",
    "    n_bins=10,\n",
    ")\n",
    "binned_coverage_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c24869",
   "metadata": {},
   "source": [
    "\n",
    "Let's make a plot to check those data visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbacc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_by_bin = binned_coverage_results.copy()\n",
    "coverage_by_bin[\"bin_label\"] = coverage_by_bin.apply(\n",
    "    lambda row: f\"[{row.bin_left:.0f}, {row.bin_right:.0f}]\", axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1069fc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = coverage_by_bin.boxplot(column=\"coverage\", by=\"bin_label\", whis=1000)\n",
    "ax.axhline(y=0.9, color=\"red\", linestyle=\"--\", label=\"Target coverage (0.9)\")\n",
    "ax.set(\n",
    "    xlabel=\"Load bins (MW)\",\n",
    "    ylabel=\"Coverage\",\n",
    "    title=\"Coverage Distribution by Load Bins\",\n",
    ")\n",
    "ax.set_title(\"Coverage Distribution by Load Bins\")\n",
    "ax.legend()\n",
    "plt.suptitle(\"\")  # Remove automatic suptitle from boxplot\n",
    "_ = plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483124b1",
   "metadata": {},
   "source": [
    "\n",
    "We observe that the lower and higher bins, so low and high load, have the worse\n",
    "coverage with a high variability.\n",
    "\n",
    "### Reliability diagrams and Lorenz curves for quantile regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cdba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_diagram(\n",
    "    cv_predictions_hgbr_50, kind=\"quantile\", quantile_level=0.50\n",
    ").interactive().properties(\n",
    "    title=\"Reliability diagram for quantile 0.50 from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4150888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_diagram(\n",
    "    cv_predictions_hgbr_05, kind=\"quantile\", quantile_level=0.05\n",
    ").interactive().properties(\n",
    "    title=\"Reliability diagram for quantile 0.05 from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ca7a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_diagram(\n",
    "    cv_predictions_hgbr_95, kind=\"quantile\", quantile_level=0.95\n",
    ").interactive().properties(\n",
    "    title=\"Reliability diagram for quantile 0.95 from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5590905",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lorenz_curve(cv_predictions_hgbr_50).interactive().properties(\n",
    "    title=\"Lorenz curve for quantile 0.50 from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b609c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lorenz_curve(cv_predictions_hgbr_05).interactive().properties(\n",
    "    title=\"Lorenz curve for quantile 0.05 from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c035a321",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plot_lorenz_curve(cv_predictions_hgbr_95).interactive().properties(\n",
    "    title=\"Lorenz curve for quantile 0.95 from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a0041c",
   "metadata": {},
   "source": [
    "\n",
    "## Quantile regression as classification\n",
    "\n",
    "In the following, we turn a quantile regression problem for all possible\n",
    "quantile levels into a multiclass classification problem by discretizing the\n",
    "target variable into bins and interpolating the cumulative sum of the bin\n",
    "membership probability to estimate the CDF of the distribution of the\n",
    "continuous target variable conditioned on the features.\n",
    "\n",
    "Ideally, the classifier should be efficient when trained on a large number of\n",
    "classes (induced by the number of bins). Therefore we use a Random Forest\n",
    "classifier as the default base estimator.\n",
    "\n",
    "There are several advantages to this approach:\n",
    "- a single model is trained and can jointly estimate quantiles for all\n",
    "  quantile levels (assuming a well tuned number of bins);\n",
    "- the quantile levels can be chosen at prediction time, which allows for a\n",
    "  flexible quantile regression model;\n",
    "- in practice, the resulting predictions are often reasonably well calibrated\n",
    "  as we will see in the reliability diagrams below.\n",
    "\n",
    "One possible drawback is that current implementations of gradient boosting\n",
    "models tend to be very slow to train with a large number of classes. Random\n",
    "Forests are much more efficient in this case, but they do not always provide\n",
    "the best predictive performance. It could be the case that combining this\n",
    "approach with tabular neural networks can lead to competitive results.\n",
    "\n",
    "However, the current scikit-learn API is not expressive enough to to handle\n",
    "the output shape of the quantile prediction function. We therefore cannot\n",
    "make it fit into a skrub pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8511b209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.utils.validation import check_consistent_length\n",
    "from sklearn.utils import check_random_state\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BinnedQuantileRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimator=None,\n",
    "        n_bins=100,\n",
    "        quantile=0.5,\n",
    "        random_state=None,\n",
    "    ):\n",
    "        self.n_bins = n_bins\n",
    "        self.estimator = estimator\n",
    "        self.quantile = quantile\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Lightweight input validation: most of the input validation will be\n",
    "        # handled by the sub estimators.\n",
    "        random_state = check_random_state(self.random_state)\n",
    "        check_consistent_length(X, y)\n",
    "        self.target_binner_ = KBinsDiscretizer(\n",
    "            n_bins=self.n_bins,\n",
    "            strategy=\"quantile\",\n",
    "            subsample=200_000,\n",
    "            encode=\"ordinal\",\n",
    "            quantile_method=\"averaged_inverted_cdf\",\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        y_binned = (\n",
    "            self.target_binner_.fit_transform(np.asarray(y).reshape(-1, 1))\n",
    "            .ravel()\n",
    "            .astype(np.int32)\n",
    "        )\n",
    "\n",
    "        # Fit the multiclass classifier to predict the binned targets from the\n",
    "        # training set.\n",
    "        if self.estimator is None:\n",
    "            estimator = RandomForestClassifier(random_state=random_state)\n",
    "        else:\n",
    "            estimator = clone(self.estimator)\n",
    "        self.estimator_ = estimator.fit(X, y_binned)\n",
    "        return self\n",
    "\n",
    "    def predict_quantiles(self, X, quantiles=(0.05, 0.5, 0.95)):\n",
    "        check_is_fitted(self, \"estimator_\")\n",
    "        edges = self.target_binner_.bin_edges_[0]\n",
    "        n_bins = edges.shape[0] - 1\n",
    "        expected_shape = (X.shape[0], n_bins)\n",
    "        y_proba_raw = self.estimator_.predict_proba(X)\n",
    "\n",
    "        # Some might stay empty on the training set. Typically, classifiers do\n",
    "        # not learn to predict an explicit 0 probability for unobserved classes\n",
    "        # so we have to post process their output:\n",
    "        if y_proba_raw.shape != expected_shape:\n",
    "            y_proba = np.zeros(shape=expected_shape)\n",
    "            y_proba[:, self.estimator_.classes_] = y_proba_raw\n",
    "        else:\n",
    "            y_proba = y_proba_raw\n",
    "\n",
    "        # Build the mapper for inverse CDF mapping, from cumulated\n",
    "        # probabilities to continuous prediction.\n",
    "        y_cdf = np.zeros(shape=(X.shape[0], edges.shape[0]))\n",
    "        y_cdf[:, 1:] = np.cumsum(y_proba, axis=1)\n",
    "        return np.asarray([interp1d(y_cdf_i, edges)(quantiles) for y_cdf_i in y_cdf])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.predict_quantiles(X, quantiles=(self.quantile,)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00e14dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = (0.05, 0.5, 0.95)\n",
    "bqr = BinnedQuantileRegressor(\n",
    "    RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        min_samples_leaf=5,\n",
    "        max_features=0.2,\n",
    "        n_jobs=-1,\n",
    "        random_state=0,\n",
    "    ),\n",
    "    n_bins=30,\n",
    ")\n",
    "bqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef09a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "X, y = features.skb.eval(), target.skb.eval()\n",
    "\n",
    "cv_results_bqr = cross_validate(\n",
    "    bqr,\n",
    "    X,\n",
    "    y,\n",
    "    cv=ts_cv_5,\n",
    "    scoring={\n",
    "        \"d2_pinball_50\": make_scorer(d2_pinball_score, alpha=0.5),\n",
    "    },\n",
    "    return_estimator=True,\n",
    "    return_indices=True,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec7c99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_predictions_bqr_all = [\n",
    "    cv_predictions_bqr_05 := [],\n",
    "    cv_predictions_bqr_50 := [],\n",
    "    cv_predictions_bqr_95 := [],\n",
    "]\n",
    "for fold_ix, (qreg, test_idx) in enumerate(\n",
    "    zip(cv_results_bqr[\"estimator\"], cv_results_bqr[\"indices\"][\"test\"])\n",
    "):\n",
    "    print(f\"CV iteration #{fold_ix}\")\n",
    "    print(f\"Test set size: {test_idx.shape[0]} rows\")\n",
    "    print(\n",
    "        f\"Test time range: {prediction_time.skb.eval()[test_idx][0, 0]} to \"\n",
    "        f\"{prediction_time.skb.eval()[test_idx][-1, 0]} \"\n",
    "    )\n",
    "    y_pred_all_quantiles = qreg.predict_quantiles(X[test_idx], quantiles=quantiles)\n",
    "\n",
    "    coverage_score = coverage(\n",
    "        y[test_idx],\n",
    "        y_pred_all_quantiles[:, 0],\n",
    "        y_pred_all_quantiles[:, 2],\n",
    "    )\n",
    "    print(f\"Coverage: {coverage_score:.3f}\")\n",
    "\n",
    "    mean_width_score = mean_width(\n",
    "        y[test_idx],\n",
    "        y_pred_all_quantiles[:, 0],\n",
    "        y_pred_all_quantiles[:, 2],\n",
    "    )\n",
    "    print(f\"Mean prediction interval width: \" f\"{mean_width_score:.1f} MW\")\n",
    "\n",
    "    for q_idx, (quantile, predictions) in enumerate(\n",
    "        zip(quantiles, cv_predictions_bqr_all)\n",
    "    ):\n",
    "        observed = y[test_idx]\n",
    "        predicted = y_pred_all_quantiles[:, q_idx]\n",
    "        predictions.append(\n",
    "            pl.DataFrame(\n",
    "                {\n",
    "                    \"prediction_time\": prediction_time.skb.eval()[test_idx],\n",
    "                    \"load_mw\": observed,\n",
    "                    \"predicted_load_mw\": predicted,\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        print(f\"d2_pinball score: {d2_pinball_score(observed, predicted):.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f02481",
   "metadata": {
    "title": "[markdown"
   },
   "outputs": [],
   "source": [
    "# Let's assess the calibration of the quantile regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb24209",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_diagram(\n",
    "    cv_predictions_bqr_50, kind=\"quantile\", quantile_level=0.50\n",
    ").interactive().properties(\n",
    "    title=\"Reliability diagram for quantile 0.50 from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1670379",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_diagram(\n",
    "    cv_predictions_bqr_05, kind=\"quantile\", quantile_level=0.05\n",
    ").interactive().properties(\n",
    "    title=\"Reliability diagram for quantile 0.05 from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4b1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_diagram(\n",
    "    cv_predictions_bqr_95, kind=\"quantile\", quantile_level=0.95\n",
    ").interactive().properties(\n",
    "    title=\"Reliability diagram for quantile 0.95 from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338db95f",
   "metadata": {},
   "source": [
    "\n",
    "We can complement this assessment with the Lorenz curves, which only assess\n",
    "the ranking power of the predictions, irrespective of their absolute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c34b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lorenz_curve(cv_predictions_bqr_50).interactive().properties(\n",
    "    title=\"Lorenz curve for quantile 0.50 from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0261595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lorenz_curve(cv_predictions_bqr_05).interactive().properties(\n",
    "    title=\"Lorenz curve for quantile 0.05 from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d92342",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lorenz_curve(cv_predictions_bqr_95).interactive().properties(\n",
    "    title=\"Lorenz curve for quantile 0.95 from cross-validation predictions\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
