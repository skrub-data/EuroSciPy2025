{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45128713",
   "metadata": {},
   "source": [
    "\n",
    "# Single horizon predictive modeling\n",
    "\n",
    "## Environment setup\n",
    "\n",
    "We need to install some extra dependencies for this notebook if needed (when\n",
    "running jupyterlite). We need the development version of skrub to be able to\n",
    "use the skrub expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b83ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q https://pypi.anaconda.org/ogrisel/simple/polars/1.24.0/polars-1.24.0-cp39-abi3-emscripten_3_1_58_wasm32.whl\n",
    "%pip install -q https://pypi.anaconda.org/ogrisel/simple/skrub/0.6.dev0/skrub-0.6.dev0-py3-none-any.whl\n",
    "%pip install -q altair holidays plotly nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fb3f76",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import altair\n",
    "import cloudpickle\n",
    "import pyarrow  # noqa: F401\n",
    "import skrub\n",
    "import tzdata  # noqa: F401\n",
    "from plotly.io import write_json, read_json  # noqa: F401\n",
    "\n",
    "from tutorial_helpers import (\n",
    "    plot_lorenz_curve,\n",
    "    plot_reliability_diagram,\n",
    "    plot_residuals_vs_predicted,\n",
    "    plot_binned_residuals,\n",
    "    collect_cv_predictions,\n",
    ")\n",
    "\n",
    "\n",
    "# Ignore warnings from pkg_resources triggered by Python 3.13's multiprocessing.\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pkg_resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f523695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"feature_engineering_pipeline.pkl\", \"rb\") as f:\n",
    "    feature_engineering_pipeline = cloudpickle.load(f)\n",
    "\n",
    "\n",
    "features = feature_engineering_pipeline[\"features\"]\n",
    "targets = feature_engineering_pipeline[\"targets\"]\n",
    "prediction_time = feature_engineering_pipeline[\"prediction_time\"]\n",
    "horizons = feature_engineering_pipeline[\"horizons\"]\n",
    "target_column_name_pattern = feature_engineering_pipeline[\"target_column_name_pattern\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8889287f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "For now, let's focus on the last horizon (24 hours) to train a model\n",
    "predicting the electricity load at the next 24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d901322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_of_interest = horizons[-1]  # Focus on the 24-hour horizon\n",
    "target_column_name = target_column_name_pattern.format(horizon=horizon_of_interest)\n",
    "predicted_target_column_name = \"predicted_\" + target_column_name\n",
    "target = targets[target_column_name].skb.mark_as_y()\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e2df84",
   "metadata": {},
   "source": [
    "\n",
    "Let's define our first single output prediction pipeline. This pipeline\n",
    "chains our previous feature engineering steps with a `skrub.DropCols` step to\n",
    "drop some columns that we do not want to use as features, and a\n",
    "`HistGradientBoostingRegressor` model from scikit-learn.\n",
    "\n",
    "The `skrub.choose_from`, `skrub.choose_float`, and `skrub.choose_int`\n",
    "functions are used to define hyperparameters that can be tuned via\n",
    "cross-validated randomized search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d8f3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import skrub.selectors as s\n",
    "\n",
    "\n",
    "features_with_dropped_cols = features.skb.apply(\n",
    "    skrub.DropCols(\n",
    "        cols=skrub.choose_from(\n",
    "            {\n",
    "                \"none\": s.glob(\"\"),  # No column has an empty name.\n",
    "                \"load\": s.glob(\"load_*\"),\n",
    "                \"rolling_load\": s.glob(\"load_mw_rolling_*\"),\n",
    "                \"weather\": s.glob(\"weather_*\"),\n",
    "                \"temperature\": s.glob(\"weather_temperature_*\"),\n",
    "                \"moisture\": s.glob(\"weather_moisture_*\"),\n",
    "                \"cloud_cover\": s.glob(\"weather_cloud_cover_*\"),\n",
    "                \"calendar\": s.glob(\"cal_*\"),\n",
    "                \"holiday\": s.glob(\"cal_is_holiday*\"),\n",
    "                \"future_1h\": s.glob(\"*_future_1h\"),\n",
    "                \"future_24h\": s.glob(\"*_future_24h\"),\n",
    "                \"non_paris_weather\": s.glob(\"weather_*\") & ~s.glob(\"weather_*_paris_*\"),\n",
    "            },\n",
    "            name=\"dropped_cols\",\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "hgbr_predictions = features_with_dropped_cols.skb.apply(\n",
    "    HistGradientBoostingRegressor(\n",
    "        random_state=0,\n",
    "        loss=skrub.choose_from([\"squared_error\", \"poisson\", \"gamma\"], name=\"loss\"),\n",
    "        learning_rate=skrub.choose_float(\n",
    "            0.01, 1, default=0.1, log=True, name=\"learning_rate\"\n",
    "        ),\n",
    "        max_leaf_nodes=skrub.choose_int(\n",
    "            3, 300, default=30, log=True, name=\"max_leaf_nodes\"\n",
    "        ),\n",
    "    ),\n",
    "    y=target,\n",
    ")\n",
    "hgbr_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e1008e",
   "metadata": {},
   "source": [
    "\n",
    "The `predictions` expression captures the whole expression graph that\n",
    "includes the feature engineering steps, the target variable, and the model\n",
    "training step.\n",
    "\n",
    "In particular, the input data keys for the full pipeline can be\n",
    "inspected as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed8732",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbr_predictions.skb.get_data().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef94bdf",
   "metadata": {},
   "source": [
    "\n",
    "Furthermore, the hyper-parameters of the full pipeline can be retrieved as\n",
    "follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27acc06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbr_pipeline = hgbr_predictions.skb.get_pipeline()\n",
    "hgbr_pipeline.describe_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94eedcd",
   "metadata": {},
   "source": [
    "\n",
    "When running this notebook locally, you can also interactively inspect all\n",
    "the steps of the DAG using the following (once uncommented):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a4ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.skb.full_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806b0f87",
   "metadata": {},
   "source": [
    "\n",
    "Since we passed input values to all the upstream `skrub` variables, `skrub`\n",
    "automatically evaluates the whole expression graph graph (train and predict\n",
    "on the same data) so that we can interactively check that everything will\n",
    "work as expected.\n",
    "\n",
    "## Assessing the model performance via cross-validation\n",
    "\n",
    "Being able to fit the training data is not enough. We need to assess the\n",
    "ability of the training pipeline to learn a predictive model that can\n",
    "generalize to unseen data.\n",
    "\n",
    "Furthermore, we want to assess the uncertainty of this estimate of the\n",
    "generalization performance via time-based cross-validation, also known as\n",
    "backtesting.\n",
    "\n",
    "scikit-learn provides a `TimeSeriesSplit` splitter providing a convenient way to\n",
    "split temporal data: in the different folds, the training data always precedes the\n",
    "test data. It implies that the size of the training data is getting larger as the\n",
    "fold index increases. The scikit-learn utility allows to define a couple of\n",
    "parameters to control the size of the training and test data and as well as a gap\n",
    "between the training and test data to potentially avoid leakage if our model relies\n",
    "on lagged features.\n",
    "\n",
    "In the example below, we define that the training data should be at most 2 years\n",
    "worth of data and the test data should be 24 weeks long. We also define a gap of\n",
    "1 week between the training.\n",
    "\n",
    "Let's check those statistics by iterating over the different folds provided by the\n",
    "splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ad90ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "max_train_size = 2 * 52 * 24 * 7  # max ~2 years of training data\n",
    "test_size = 24 * 7 * 24  # 24 weeks of test data\n",
    "gap = 7 * 24  # 1 week gap between train and test sets\n",
    "ts_cv_5 = TimeSeriesSplit(\n",
    "    n_splits=5, max_train_size=max_train_size, test_size=test_size, gap=gap\n",
    ")\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(\n",
    "    ts_cv_5.split(prediction_time.skb.eval())\n",
    "):\n",
    "    print(f\"CV iteration #{fold_idx}\")\n",
    "    train_datetimes = prediction_time.skb.eval()[train_idx]\n",
    "    test_datetimes = prediction_time.skb.eval()[test_idx]\n",
    "    print(\n",
    "        f\"Train: {train_datetimes.shape[0]} rows, \"\n",
    "        f\"Test: {test_datetimes.shape[0]} rows\"\n",
    "    )\n",
    "    print(f\"Train time range: {train_datetimes[0, 0]} to \" f\"{train_datetimes[-1, 0]} \")\n",
    "    print(f\"Test time range: {test_datetimes[0, 0]} to \" f\"{test_datetimes[-1, 0]} \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4147e2bb",
   "metadata": {},
   "source": [
    "\n",
    "Once the cross-validation strategy is defined, we pass it to the\n",
    "`cross_validate` function provided by `skrub` to compute the cross-validated\n",
    "scores. Here, we compute the mean absolute percentage error that is easily\n",
    "interpretable and customary for regression tasks with a strictly positive\n",
    "target variable such as electricity load forecasting.\n",
    "\n",
    "We can also look at the R2 score and the Poisson and Gamma deviance which are\n",
    "all strictly proper scoring rules for estimation of E[y|X]: in the large\n",
    "sample limit, minimizers of those metrics all identify the conditional\n",
    "expectation of the target variable given the features for strictly positive\n",
    "target variables. All those metrics follow the higher is better convention,\n",
    "1.0 is the maximum reachable score and 0.0 is the score of a model that\n",
    "predicts the mean of the target variable for all observations, irrespective\n",
    "of the features.\n",
    "\n",
    "No that in general, a deviance score of 1.0 is not reachable since it\n",
    "corresponds to a model that always predicts the target value exactly\n",
    "for all observations. In practice, because there is always a fraction of the\n",
    "variability in the target variable that is not explained by the information\n",
    "available to construct the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1beeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, mean_absolute_percentage_error, get_scorer\n",
    "from sklearn.metrics import d2_tweedie_score\n",
    "\n",
    "\n",
    "hgbr_cv_results = hgbr_predictions.skb.cross_validate(\n",
    "    cv=ts_cv_5,\n",
    "    scoring={\n",
    "        \"mape\": make_scorer(mean_absolute_percentage_error),\n",
    "        \"r2\": get_scorer(\"r2\"),\n",
    "        \"d2_poisson\": make_scorer(d2_tweedie_score, power=1.0),\n",
    "        \"d2_gamma\": make_scorer(d2_tweedie_score, power=2.0),\n",
    "    },\n",
    "    return_train_score=True,\n",
    "    return_pipeline=True,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "hgbr_cv_results.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f2964d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "Those results show very good performance of the model: less than 3% of mean\n",
    "absolute percentage error (MAPE) on the test folds. Similarly, all the\n",
    "deviance scores are close to 1.0.\n",
    "\n",
    "We observe a bit of variability in the scores across the different folds: in\n",
    "particular the test performance on the first fold seems to be worse than the\n",
    "other folds. This is likely due to the fact that the first fold contains\n",
    "training data from 2021 and 2022 and the test data mostly from 2023.\n",
    "\n",
    "The invasion in Ukraine and a sharp drop in nuclear electricity production\n",
    "due to safety problems strongly impacted the distribution of the electricity\n",
    "prices in 2022, with unprecedented high prices, which can in turn cause a\n",
    "shift in the electricity load demand. This could explain a higher than usual\n",
    "distribution shift between the train and test folds of the first CV\n",
    "iteration.\n",
    "\n",
    "We can further refine the analysis of the performance of our model by\n",
    "collecting the predictions on each cross-validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77398d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbr_cv_predictions = collect_cv_predictions(\n",
    "    hgbr_cv_results[\"pipeline\"], ts_cv_5, hgbr_predictions, prediction_time\n",
    ")\n",
    "hgbr_cv_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c407cf7a",
   "metadata": {},
   "source": [
    "\n",
    "As a sanity check, we will take a look at the predictions on the first fold and plot\n",
    "the observed values and the prediction values from the model. We limit the\n",
    "visualization to the last 7 days of the fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2a3240",
   "metadata": {},
   "outputs": [],
   "source": [
    "altair.Chart(\n",
    "    hgbr_cv_predictions[0].tail(24 * 7)\n",
    ").transform_fold(\n",
    "    [\"load_mw\", \"predicted_load_mw\"],\n",
    ").mark_line(\n",
    "    tooltip=True\n",
    ").encode(\n",
    "    x=\"prediction_time:T\", y=\"value:Q\", color=\"key:N\"\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0536e0",
   "metadata": {},
   "source": [
    "\n",
    "Now, let's check the performance of our models.\n",
    "\n",
    "The first curve is called the Lorenz curve. It shows on the x-axis the fraction of\n",
    "observations sorted by predicted values and on the y-axis the cumulative observed\n",
    "load proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a20b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lorenz_curve(hgbr_cv_predictions).interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabe7a24",
   "metadata": {},
   "source": [
    "\n",
    "The diagonal on the plot corresponds to a model predicting a constant value that is\n",
    "therefore not an informative model. The oracle model corresponds to the \"perfect\"\n",
    "model that would provide the an output identical to the observed values. Thus, the\n",
    "ranking of such hypothetical model is the best possible ranking. However, you should\n",
    "note that the oracle model is not the line passing through the right-hand corner of\n",
    "the plot. Instead, this curvature is defined by the distribution of the observations.\n",
    "Indeed, more the observations are composed of small values and a couple of large\n",
    "values, the more the oracle model is closer to the right-hand corner of the plot.\n",
    "\n",
    "A true model is navigating between the diagonal and the oracle model. The area between\n",
    "the diagonal and the Lorenz curve of a model is called the Gini index.\n",
    "\n",
    "For our model, we observe that each oracle model is not far from the diagonal. It\n",
    "means that the observed values do not contain a couple of large values with high\n",
    "variability. Therefore, it informs us that the complexity of our problem at hand is\n",
    "not too high. Looking at the Lorenz curve of each model, we observe that it is quite\n",
    "close to the oracle model. Therefore, the gradient boosting regressor is\n",
    "discriminative for our task.\n",
    "\n",
    "Then, we have a look at the reliability diagram. This diagram shows on the x-axis the\n",
    "mean predicted load and on the y-axis the mean observed load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04eeaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_diagram(hgbr_cv_predictions).interactive().properties(\n",
    "    title=\"Reliability diagram from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d68dab",
   "metadata": {},
   "source": [
    "\n",
    "The diagonal on the reliability diagram corresponds to the best possible model: for\n",
    "a level of predicted load that fall into a bin, then the mean observed load is also\n",
    "in the same bin. If the line is above the diagonal, it means that our model is\n",
    "predicted a value too low in comparison to the observed values. If the line is below\n",
    "the diagonal, it means that our model is predicted a value too high in comparison to\n",
    "the observed values.\n",
    "\n",
    "For our cross-validated model, we observe that each reliability curve is close to the\n",
    "diagonal. We only observe a mis-calibration for the extremum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c7caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals_vs_predicted(hgbr_cv_predictions).interactive().properties(\n",
    "    title=\"Residuals vs Predicted Values from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9788f84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_binned_residuals(hgbr_cv_predictions, by=\"hour\").interactive().properties(\n",
    "    title=\"Residuals by hour of the day from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc22be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_binned_residuals(hgbr_cv_predictions, by=\"month\").interactive().properties(\n",
    "    title=\"Residuals by hour of the day from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74c1e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_cv_2 = TimeSeriesSplit(\n",
    "    n_splits=2, test_size=test_size, max_train_size=max_train_size, gap=24\n",
    ")\n",
    "# randomized_search_hgbr = hgbr_predictions.skb.get_randomized_search(\n",
    "#     cv=ts_cv_2,\n",
    "#     scoring=\"r2\",\n",
    "#     n_iter=100,\n",
    "#     fitted=True,\n",
    "#     verbose=1,\n",
    "#     n_jobs=-1,\n",
    "# )\n",
    "# # %%\n",
    "# randomized_search_hgbr.results_.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a555c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = randomized_search_hgbr.plot_results().update_layout(margin=dict(l=200))\n",
    "# write_json(fig, \"parallel_coordinates_hgbr.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264d29d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = read_json(\"parallel_coordinates_hgbr.json\")\n",
    "fig.update_layout(margin=dict(l=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb423d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nested_cv_results = skrub.cross_validate(\n",
    "#     environment=predictions.skb.get_data(),\n",
    "#     pipeline=randomized_search,\n",
    "#     cv=ts_cv_5,\n",
    "#     scoring={\n",
    "#         \"r2\": get_scorer(\"r2\"),\n",
    "#         \"mape\": make_scorer(mean_absolute_percentage_error),\n",
    "#     },\n",
    "#     n_jobs=-1,\n",
    "#     return_pipeline=True,\n",
    "# ).round(3)\n",
    "# nested_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9e589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for outer_fold_idx in range(len(nested_cv_results)):\n",
    "#     print(\n",
    "#         nested_cv_results.loc[outer_fold_idx, \"pipeline\"]\n",
    "#         .results_.loc[0]\n",
    "#         .round(3)\n",
    "#         .to_dict()\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df48c8cf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "### Exercise: non-linear feature engineering coupled with linear predictive model\n",
    "\n",
    "Now, it is your turn to make a predictive model. Towards this end, we request you\n",
    "to preprocess the input features with non-linear feature engineering:\n",
    "\n",
    "- the first step is to impute the missing values using a `SimpleImputer`. Make sure\n",
    "  to include the indicator of missing values in the feature set (i.e. look at the\n",
    "  `add_indicator` parameter);\n",
    "- use a `SplineTransformer` to create non-linear features. Use the default parameters\n",
    "  but make sure to set `sparse_output=True` since it subsequent processing will be\n",
    "  faster and more memory efficient with such data structure;\n",
    "- use a `VarianceThreshold` to remove features with potential constant features;\n",
    "- use a `SelectKBest` to select the most informative features. Set `k` to be chosen\n",
    "  from a log-uniform distribution between 100 and 1,000 (i.e. use `skrub.choose_int`);\n",
    "- use a `Nystroem` to approximate an RBF kernel. Set `n_components` to be chosen\n",
    "  from a log-uniform distribution between 10 and 200 (i.e. use `skrub.choose_int`).\n",
    "- finally, use a `Ridge` as the final predictive model. Set `alpha` to be\n",
    "  chosen from a log-uniform distribution between 1e-6 and 1e3 (i.e. use\n",
    "  `skrub.choose_float`).\n",
    "\n",
    "Use a scikit-learn `Pipeline` using `make_pipeline` to chain the steps together.\n",
    "\n",
    "Once the predictive model is defined, apply it on the `feature_with_dropped_cols`\n",
    "expression. Do not forget to define that `target` is the `y` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc29266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we provide all the imports for creating the predictive model.\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import SplineTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6036087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3d936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ridge = features_with_dropped_cols.skb.apply(\n",
    "    make_pipeline(\n",
    "        SimpleImputer(add_indicator=True),\n",
    "        SplineTransformer(sparse_output=True),\n",
    "        VarianceThreshold(threshold=1e-6),\n",
    "        SelectKBest(\n",
    "            k=skrub.choose_int(100, 1_000, log=True, name=\"n_selected_splines\")\n",
    "        ),\n",
    "        Nystroem(\n",
    "            n_components=skrub.choose_int(\n",
    "                10, 200, log=True, name=\"n_components\", default=150\n",
    "            )\n",
    "        ),\n",
    "        Ridge(\n",
    "            alpha=skrub.choose_float(1e-6, 1e3, log=True, name=\"alpha\", default=1e-2)\n",
    "        ),\n",
    "    ),\n",
    "    y=target,\n",
    ")\n",
    "predictions_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c093b434",
   "metadata": {},
   "source": [
    "\n",
    "Now that you defined the predictive model, let's make a similar analysis than earlier.\n",
    "Let's evaluate the performance of the model using cross-validation. Use the\n",
    "time-based cross-validation splitter `ts_cv_5` defined earlier. Make sure to compute\n",
    "the R2 score and the mean absolute percentage error. Return the training scores as\n",
    "well as the fitted pipeline such that we can make additional analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c3bf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a899e96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_ridge = predictions_ridge.skb.cross_validate(\n",
    "    cv=ts_cv_5,\n",
    "    scoring={\n",
    "        \"r2\": get_scorer(\"r2\"),\n",
    "        \"mape\": make_scorer(mean_absolute_percentage_error),\n",
    "    },\n",
    "    return_train_score=True,\n",
    "    return_pipeline=True,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb2c6f8",
   "metadata": {},
   "source": [
    "Do a sanity check by plotting the observed values and predictions for the first fold\n",
    "as we did earlier.\n",
    "\n",
    "Then, make an analysis of the cross-validated metrics.\n",
    "Does this model perform better or worse than the previous model?\n",
    "Is it underfitting or overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3323513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5fa407",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_ridge.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20feebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_predictions_ridge = collect_cv_predictions(\n",
    "    cv_results_ridge[\"pipeline\"], ts_cv_5, predictions_ridge, prediction_time\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05085cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "altair.Chart(cv_predictions_ridge[0].tail(24 * 7)).transform_fold(\n",
    "    [\"load_mw\", \"predicted_load_mw\"],\n",
    ").mark_line(\n",
    "    tooltip=True\n",
    ").encode(\n",
    "    x=\"prediction_time:T\", y=\"value:Q\", color=\"key:N\"\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f46f84",
   "metadata": {},
   "source": [
    "\n",
    "Compute the Lorenz curve and the reliability diagram for this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bd9173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831b7c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lorenz_curve(cv_predictions_ridge).interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87e2b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_diagram(cv_predictions_ridge).interactive().properties(\n",
    "    title=\"Reliability diagram from cross-validation predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c188e7d",
   "metadata": {},
   "source": [
    "\n",
    "Now, let's perform a randomized search on the hyper-parameters of the model. The code\n",
    "to perform the search is shown below. Since it will be pretty computationally\n",
    "expensive, we are reloading the results of the parallel coordinates plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8397053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomized_search_ridge = predictions_ridge.skb.get_randomized_search(\n",
    "#     cv=ts_cv_2,\n",
    "#     scoring=\"r2\",\n",
    "#     n_iter=100,\n",
    "#     fitted=True,\n",
    "#     verbose=1,\n",
    "#     n_jobs=-1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed29e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = randomized_search_ridge.plot_results().update_layout(margin=dict(l=200))\n",
    "# write_json(fig, \"parallel_coordinates_ridge.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca8ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = read_json(\"parallel_coordinates_ridge.json\")\n",
    "fig.update_layout(margin=dict(l=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e4a0b0",
   "metadata": {},
   "source": [
    "\n",
    "We observe that the default values of the hyper-parameters are in the optimal\n",
    "region explored by the randomized search. This is a good sign that the model\n",
    "is well-specified and that the hyper-parameters are not too sensitive to\n",
    "small changes of those values.\n",
    "\n",
    "We could further assess the stability of those optimal hyper-parameters by\n",
    "running a nested cross-validation, where we would perform a randomized search\n",
    "for each fold of the outer cross-validation loop as below but this is\n",
    "computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294d617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nested_cv_results_ridge = skrub.cross_validate(\n",
    "#     environment=predictions_ridge.skb.get_data(),\n",
    "#     pipeline=randomized_search_ridge,\n",
    "#     cv=ts_cv_5,\n",
    "#     scoring={\n",
    "#         \"r2\": get_scorer(\"r2\"),\n",
    "#         \"mape\": make_scorer(mean_absolute_percentage_error),\n",
    "#     },\n",
    "#     n_jobs=-1,\n",
    "#     return_pipeline=True,\n",
    "# ).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfc1e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nested_cv_results_ridge.round(3)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
