{
  "hash": "10674a21969e89d5c84b908c947813ea",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Skrub: ML with Dataframes\"\ntitle-block-banner: true\ndate: 2025-08-18\nsubtitle: \"Tutorial at EuroSciPy 2025\"\nauthor: \"Riccardo Cappuzzo, Guillaume Lemaitre\"\nformat: \n    revealjs:\n        slide-number: c/t\n        show-slide-number: all\n        preview-links: auto\n        embed-resources: false\n        transition: slide\n        theme: [simple]\n        logo: images/skrub.svg\n        css: style.css\nincremental: true\n\n---\n\n# Program for the tutorial\n\n1. Short introduction to `skrub`\n2. Simple `skrub` transformers\n3. Starting with the `skrub` DataOps\n4. The tutorial! **Forecasting with `skrub` DataOps and Polars**\n\n# Boost your productivity with `skrub`! {auto-animate=\"true\"}\n\n`skrub` simplifies many tedious data preparation operations\n\n## An example pipeline\n1. Gather some data\n2. Explore the data\n3. Pre-process the data \n4. Perform feature engineering\n5. Build a scikit-learn pipeline\n6. ???\n7. Profit?  \n\n## Exploring the data with `skrub` {.smaller auto-animate=\"true\"}\n\n```{.python}\nfrom skrub import TableReport\nTableReport(employee_salaries)\n```\n[Preview](https://skrub-data.org/skrub-reports/examples/employee_salaries.html){preview-link=\"true\"}\n\n\n::: {.fragment}\n::: {.nonincremental}\nMain features:\n\n- Obtain high-level statistics about the data\n- Explore the distribution of values and find outliers\n- Discover highly correlated columns \n- Export and share the report as an `html` file\n:::\n:::\n\n## Lightweight data cleaning: `Cleaner` {.smaller auto-animate=\"true\"}\n\n::: {#f7e286bd .cell execution_count=1}\n``` {.python .cell-code}\nfrom skrub.datasets import fetch_employee_salaries\nfrom pprint import pprint\nimport pandas as pd\n\ndataset = fetch_employee_salaries()\nemployees, salaries = dataset.X, dataset.y\n\ndf = pd.DataFrame(employees)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDownloading 'employee_salaries' from https://github.com/skrub-data/skrub-data-files/raw/refs/heads/main/employee_salaries.zip (attempt 1/3)\n```\n:::\n:::\n\n\n## Lightweight data cleaning: `Cleaner` {.smaller auto-animate=\"true\"}\n\n::: {#e4b33985 .cell execution_count=2}\n``` {.python .cell-code}\nfrom skrub import Cleaner\ncleaner = Cleaner(drop_if_constant=True, datetime_format='%d/%m/%Y')\ndf_cleaned = cleaner.fit_transform(df)\ndisplay(df_cleaned)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gender</th>\n      <th>department</th>\n      <th>department_name</th>\n      <th>division</th>\n      <th>assignment_category</th>\n      <th>employee_position_title</th>\n      <th>date_first_hired</th>\n      <th>year_first_hired</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>F</td>\n      <td>POL</td>\n      <td>Department of Police</td>\n      <td>MSB Information Mgmt and Tech Division Records...</td>\n      <td>Fulltime-Regular</td>\n      <td>Office Services Coordinator</td>\n      <td>09/22/1986</td>\n      <td>1986</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>M</td>\n      <td>POL</td>\n      <td>Department of Police</td>\n      <td>ISB Major Crimes Division Fugitive Section</td>\n      <td>Fulltime-Regular</td>\n      <td>Master Police Officer</td>\n      <td>09/12/1988</td>\n      <td>1988</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>F</td>\n      <td>HHS</td>\n      <td>Department of Health and Human Services</td>\n      <td>Adult Protective and Case Management Services</td>\n      <td>Fulltime-Regular</td>\n      <td>Social Worker IV</td>\n      <td>11/19/1989</td>\n      <td>1989</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>M</td>\n      <td>COR</td>\n      <td>Correction and Rehabilitation</td>\n      <td>PRRS Facility and Security</td>\n      <td>Fulltime-Regular</td>\n      <td>Resident Supervisor II</td>\n      <td>05/05/2014</td>\n      <td>2014</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>M</td>\n      <td>HCA</td>\n      <td>Department of Housing and Community Affairs</td>\n      <td>Affordable Housing Programs</td>\n      <td>Fulltime-Regular</td>\n      <td>Planning Specialist III</td>\n      <td>03/05/2007</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9223</th>\n      <td>F</td>\n      <td>HHS</td>\n      <td>Department of Health and Human Services</td>\n      <td>School Based Health Centers</td>\n      <td>Fulltime-Regular</td>\n      <td>Community Health Nurse II</td>\n      <td>11/03/2015</td>\n      <td>2015</td>\n    </tr>\n    <tr>\n      <th>9224</th>\n      <td>F</td>\n      <td>FRS</td>\n      <td>Fire and Rescue Services</td>\n      <td>Human Resources Division</td>\n      <td>Fulltime-Regular</td>\n      <td>Fire/Rescue Division Chief</td>\n      <td>11/28/1988</td>\n      <td>1988</td>\n    </tr>\n    <tr>\n      <th>9225</th>\n      <td>M</td>\n      <td>HHS</td>\n      <td>Department of Health and Human Services</td>\n      <td>Child and Adolescent Mental Health Clinic Serv...</td>\n      <td>Parttime-Regular</td>\n      <td>Medical Doctor IV - Psychiatrist</td>\n      <td>04/30/2001</td>\n      <td>2001</td>\n    </tr>\n    <tr>\n      <th>9226</th>\n      <td>M</td>\n      <td>CCL</td>\n      <td>County Council</td>\n      <td>Council Central Staff</td>\n      <td>Fulltime-Regular</td>\n      <td>Manager II</td>\n      <td>09/05/2006</td>\n      <td>2006</td>\n    </tr>\n    <tr>\n      <th>9227</th>\n      <td>M</td>\n      <td>DLC</td>\n      <td>Department of Liquor Control</td>\n      <td>Licensure, Regulation and Education</td>\n      <td>Fulltime-Regular</td>\n      <td>Alcohol/Tobacco Enforcement Specialist II</td>\n      <td>01/30/2012</td>\n      <td>2012</td>\n    </tr>\n  </tbody>\n</table>\n<p>9228 rows Ã— 8 columns</p>\n</div>\n```\n:::\n:::\n\n\n## Encoding datetime features `skrub.DatetimeEncoder` {auto-animate=\"true\" .smaller}\n\n::: {#8f26c4e9 .cell execution_count=3}\n``` {.python .cell-code}\nfrom skrub import DatetimeEncoder, ToDatetime\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'Constant int': [1, 1, 1],  # Single unique value\n    'B': [2, 3, 2],  # Multiple unique values\n    'Constant str': ['x', 'x', 'x'],  # Single unique value\n    'D': [4, 5, 6],  # Multiple unique values\n    'All nan': [np.nan, np.nan, np.nan],  # All missing values \n    'All empty': ['', '', ''],  # All empty strings\n    'Date': ['01/01/2023', '02/01/2023', '03/01/2023'],\n}\n\ndf = pd.DataFrame(data)\nde = DatetimeEncoder(periodic_encoding=\"circular\")\nX_date = ToDatetime().fit_transform(df[\"Date\"])\nX_enc = de.fit_transform(X_date)\nprint(X_enc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Date_year  Date_total_seconds  Date_month_circular_0  \\\n0     2023.0        1.672531e+09               0.500000   \n1     2023.0        1.675210e+09               0.866025   \n2     2023.0        1.677629e+09               1.000000   \n\n   Date_month_circular_1  Date_day_circular_0  Date_day_circular_1  \n0           8.660254e-01             0.207912             0.978148  \n1           5.000000e-01             0.207912             0.978148  \n2           6.123234e-17             0.207912             0.978148  \n```\n:::\n:::\n\n\n## What periodic features look like\n![](images/periodic_features.png){fig-align=\"center\"}\n\n\n## Encoding _all the features_ with `skrub.TableVectorizer` {.smaller auto-animate=\"true\"}\n\n![](images/skrub-table-vectorizer.png)\n\n## Build a predictive pipeline {auto-animate=\"true\" visibility=\"uncounted\"}\n```{.python }\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\n\ncategorical_columns = selector(dtype_include=object)(employees)\nnumerical_columns = selector(dtype_exclude=object)(employees)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\n\nmodel = make_pipeline(ct, SimpleImputer(), Ridge())\n```\n## Build a predictive pipeline with `tabular_pipeline` {auto-animate=\"true\"}\n\n::: {#885e8e5c .cell execution_count=4}\n``` {.python .cell-code}\nimport skrub\nfrom sklearn.linear_model import Ridge\nmodel = skrub.tabular_pipeline(Ridge())\n```\n:::\n\n\n## We now have a pipeline! {.smaller}\n\n1. Gather some data\n    - `skrub.datasets`, or user data\n2. Explore the data\n    - `skrub.TableReport`\n3. Pre-process the data \n    - `skrub.TableVectorizer`, `Cleaner`, ... \n4. Perform feature engineering\n    - `DatetimeEncoder`, `TextEncoder`, `StringEncoder `...\n5. Build a scikit-learn pipeline\n    - `tabular_pipeline`, `sklearn.pipeline.make_pipeline` ... \n6. ???\n7. Profit ðŸ“ˆ \n\n\n# What if we had a *better* pipeline? \n\n## A realistic scenario\nA data scientist needs to train a ML model, but features are spread across \nmultiple tables. \n\n::: {.fragment}\n::: {.callout-warning}\nMany issues with this! \n:::\n\n:::\n\n::: {.incremental}\n- `scikit-learn` pipelines support only a single feature matrix `X`\n- Dataframe operations cannot be tuned\n- Data leakage must be accounted for\n- Persisting and reproducing operations is complex\n:::\n\n## `skrub` DataOps\nWhen a normal pipe is not enough...\n\n::: {.fragment style=\"font-size:2em;\"}\n... the `skrub` DataOps come to the rescue ðŸš’\n:::\n\n## DataOps...\n- Extend the `scikit-learn` machinery to complex multi-table operations\n- Track all operations with a computational graph\n- Can tune any operation in the data plan\n- Can be persisted and shared easily by generating a `learner`\n\n## DataOps, DataOps plans, `learner`s: oh my!  \n- A `DataOp` (singular) wraps a single operation, and can be combined and concatenated with other `DataOps`. \n\n- We refer to a sequence and combination of `DataOps` as a `DataOps` plan.  \n\n- The `DataOps` plan can be exported as a standalone object called `learner`. The `learner` takes a dictionary of values rather than just `X` and `y`. \n\n\n## How do DataOps work, though? \nDataOps **wrap** around *user operations*, where user operations are:\n\n- any dataframe operation (e.g., merge, group by, aggregate etc.)\n- scikit-learn estimators (a Random Forest, RidgeCV etc.)\n- custom user code (load data from a path, fetch from an URL etc.)\n\n::: {.fragment}\n\n::: {.callout-important}\nDataOps _record_ user operations, so that they can later be _replayed_ in the same\norder and with the same arguments on unseen data. \n:::\n::: \n\n\n## Starting with the `DataOps`\n\n::: {#b3e91187 .cell execution_count=5}\n``` {.python .cell-code}\ndata = skrub.datasets.fetch_credit_fraud()\n\nbaskets = skrub.var(\"baskets\", data.baskets)\nX = baskets[[\"ID\"]].skb.mark_as_X()\ny = baskets[\"fraud_flag\"].skb.mark_as_y()\n\nproducts = skrub.var(\"products\", data.products) # add a new variable\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDownloading 'credit_fraud' from https://github.com/skrub-data/skrub-data-files/raw/refs/heads/main/credit_fraud.zip (attempt 1/3)\n```\n:::\n:::\n\n\n:::{.incremental}\n- `X`, `y`, `products` represent inputs to the pipeline.\n- `skrub` splits `X` and `y` when training. \n:::\n\n##  Building a full data plan\n```{.python}\nfrom skrub import selectors as s\nfrom sklearn.ensemble import ExtraTreesClassifier  \n\nvectorizer = skrub.TableVectorizer(high_cardinality=skrub.StringEncoder(), n_jobs=-1)\n\nvectorized_products = products.skb.apply(vectorizer, cols=s.all() - \"basket_ID\")\naggregated_products = vectorized_products.groupby(\"basket_ID\").agg(\"mean\").reset_index()\n\nfeatures = X.merge(aggregated_products, left_on=\"ID\", right_on=\"basket_ID\")\nfeatures = features.drop(columns=[\"ID\", \"basket_ID\"])\n\npredictions = features.skb.apply(ExtraTreesClassifier(n_jobs=-1), y=y)\n```\n\n## Exporting the plan in a `learner` {.smaller}\nThe data plan can be exported as a `learner`:\n```{.python}\n# anywhere\nlearner = predictions.skb.make_learner()\n# search is a HPO object\nbest_learner = search.skb.best_learner_\n```\n::: {.fragment}\nThen, the `learner` can be pickled ...\n```{.python}\nimport pickle\n\nwith open(\"learner.bin\", \"wb\") as fp:\n    pickle.dump(learner, fp)\n```\n:::\n\n::: {.fragment}\n\n... and loaded\n\n```{.python}\nwith open(\"learner.bin\", \"rb\") as fp:\n    learner = pickle.load(fp)\n\nlearner.predict({\"baskets\": new_baskets, \"products\": new_products})\n```\n:::\n\n\n## Hyperparameter tuning in a Data Plan \n`skrub` implements four `choose_*` functions (and `optional`):\n\n- `choose_from`: select from the given list of options\n- `choose_int`: select an integer within a range\n- `choose_float`: select a float within a range\n- `choose_bool`: select a bool \n- `optional`: chooses between a value or DataOp and no op\n\n\n## Hyperparameter tuning in a Data Plan  {auto-animate=\"true\"}\nIt's possible to nest these functions to create complex grids:\n```python\nX.skb.apply(\n    skrub.choose_from(\n        {\n            \"PCA\": PCA(n_components=skrub.choose_int(10, 30)),\n            \"SelectKBest\": SelectKBest(k=skrub.choose_int(10, 30))\n        }, name=\"dim_reduction\"\n    )\n)\n```\n\n## Observe the impact of the hyperparameters {auto-animate=\"true\"} \n\n```{.python}\nsearch = pred.skb.get_randomized_search(scoring=\"roc_auc\", fitted=True)\n\nsearch.plot_parallel_coord()\n```\n\n![](images/plot-parallel-coord.png){fig-align=\"center\"}\n\n\n#  Getting involved\n::: {.nonincremental}\n- [Skrub website](https://skrub-data.org/stable/) (QR code below!)\n- [Skrub materials website](https://skrub-data.org/skrub-materials/index.html)\n- [Git repository](https://github.com/skrub-data/skrub/)\n- [Discord server](https://discord.gg/ABaPnm7fDC)\n- [Bluesky](https://bsky.app/profile/skrub-data.bsky.social)\n:::\n\n![](images/qr-code.png){.absolute bottom=0 right=0 width=\"250\" height=\"250\"}\n\n# Time for the tutorial ...\n\n",
    "supporting": [
      "slides_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}